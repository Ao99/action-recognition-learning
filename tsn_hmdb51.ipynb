{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is copied from\n",
    "\n",
    "https://gluon-cv.mxnet.io/build/examples_action_recognition/dive_deep_tsn_ucf101.html \n",
    "\n",
    "and modified to be used on the HMDB51 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dive Deep into Training TSN mdoels on HMDB51\n",
    "==================================================\n",
    "\n",
    "This is a video action recognition tutorial using Gluon CV toolkit, a step-by-step example.\n",
    "The readers should have basic knowledge of deep learning and should be familiar with Gluon API.\n",
    "New users may first go through `A 60-minute Gluon Crash Course <http://gluon-crash-course.mxnet.io/>`_.\n",
    "You can `Start Training Now`_ or `Dive into Deep`_.\n",
    "\n",
    "Start Training Now\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "Network Structure\n",
    "-----------------\n",
    "\n",
    "First, let's import the necessary libraries into python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, os, sys, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluoncv as gcv\n",
    "from mxnet import gluon, nd, init, context\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.data.transforms import video\n",
    "from gluoncv.data import HMDB51\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, LRSequential, LRScheduler, split_and_load, TrainingHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video action recognition is a classification problem.\n",
    "Here we pick a simple yet well-performing structure, ``resnet50_v1b_hmdb51``, for the\n",
    "tutorial. In addition, we use the the idea of temporal segments (TSN) [Wang16]_\n",
    "to wrap the backbone VGG16 network for adaptation to video domain.\n",
    "\n",
    "`TSN <https://arxiv.org/abs/1608.00859>`_ is a widely adopted video\n",
    "classification method. It is proposed to incorporate temporal information from an entire video.\n",
    "The idea is straightforward: we can evenly divide the video into several segments,\n",
    "process each segment individually, obtain segmental consensus from each segment, and perform\n",
    "final prediction. TSN is more like a general algorithm, rather than a specific network architecture.\n",
    "It can work with both 2D and 3D neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActionRecResNetV1b(\n",
      "  (conv1): Conv2D(3 -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "  (relu): Activation(relu)\n",
      "  (maxpool): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
      "  (layer1): HybridSequential(\n",
      "    (0): BottleneckV1b(\n",
      "      (conv1): Conv2D(64 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu3): Activation(relu)\n",
      "      (downsample): HybridSequential(\n",
      "        (0): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckV1b(\n",
      "      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (2): BottleneckV1b(\n",
      "      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "  )\n",
      "  (layer2): HybridSequential(\n",
      "    (0): BottleneckV1b(\n",
      "      (conv1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu3): Activation(relu)\n",
      "      (downsample): HybridSequential(\n",
      "        (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckV1b(\n",
      "      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (2): BottleneckV1b(\n",
      "      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (3): BottleneckV1b(\n",
      "      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "  )\n",
      "  (layer3): HybridSequential(\n",
      "    (0): BottleneckV1b(\n",
      "      (conv1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "      (downsample): HybridSequential(\n",
      "        (0): Conv2D(512 -> 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (2): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (3): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (4): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (5): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "  )\n",
      "  (layer4): HybridSequential(\n",
      "    (0): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      (relu3): Activation(relu)\n",
      "      (downsample): HybridSequential(\n",
      "        (0): Conv2D(1024 -> 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckV1b(\n",
      "      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (2): BottleneckV1b(\n",
      "      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCHW)\n",
      "  (flat): Flatten\n",
      "  (drop): Dropout(p = 0.9, axes=())\n",
      "  (output): Dense(2048 -> 51, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# Get the model vgg16_hmdb51 with temporal segment network, with 101 output classes, without pre-trained weights\n",
    "net = get_model(name='resnet50_v1b_hmdb51', nclass=51, num_segments=3)\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation and Data Loader\n",
    "---------------------------------\n",
    "\n",
    "Data augmentation for video is different from image. For example, if you\n",
    "want to randomly crop a video sequence, you need to make sure all the video\n",
    "frames in this sequence undergo the same cropping process. We provide a\n",
    "new set of transformation functions, working with multiple images.\n",
    "Please checkout the `video.py <../../../gluoncv/data/transforms/video.py>`_ for more details.\n",
    "Most video data augmentation strategies used here are introduced in [Wang15]_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # Fix the input video frames size as 256×340 and randomly sample the cropping width and height from\n",
    "    # {256,224,192,168}. After that, resize the cropped regions to 224 × 224.\n",
    "    video.VideoMultiScaleCrop(size=(224, 224), scale_ratios=[1.0, 0.875, 0.75, 0.66]),\n",
    "    # Randomly flip the video frames horizontally\n",
    "    video.VideoRandomHorizontalFlip(),\n",
    "    # Transpose the video frames from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    video.VideoToTensor(),\n",
    "    # Normalize the video frames with mean and standard deviation calculated across all images\n",
    "    video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the transform functions, we can define data loaders for our\n",
    "training datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 3570 training samples.\n"
     ]
    }
   ],
   "source": [
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 15\n",
    "# Number of data loader workers\n",
    "num_workers = 4\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training the model. Here we set num_segments to 3 to enable TSN training.\n",
    "train_dataset = HMDB51(train=True, num_segments=3, transform=transform_train)\n",
    "print('Load %d training samples.' % len(train_dataset))\n",
    "train_data = gluon.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                   shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, Loss and Metric\n",
    "--------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [30, 50, np.inf]\n",
    "\n",
    "# Stochastic gradient descent\n",
    "optimizer = 'sgd'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.01, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize our model, we need a loss function.\n",
    "For classification tasks, we usually use softmax cross entropy as the\n",
    "loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we use accuracy as the metric to monitor our training\n",
    "process. Besides, we record metric values, and will print them at the\n",
    "end of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "After all the preparations, we can finally start training!\n",
    "Following is the script.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In order to finish the tutorial quickly, we only train for 3 epochs, and 100 iterations per epoch.\n",
    "  In your experiments, we recommend setting ``epochs=80`` for the full HMDB51 dataset.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 0] train=0.081924 loss=3.767400 time: 90.525495\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 1] train=0.103483 loss=3.586216 time: 90.619416\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 2] train=0.157546 loss=3.261760 time: 92.949263\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 3] train=0.205970 loss=3.056516 time: 93.469467\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 4] train=0.242454 loss=2.870195 time: 93.595053\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 5] train=0.292869 loss=2.660094 time: 93.795719\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 6] train=0.313101 loss=2.577416 time: 93.893380\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 7] train=0.349254 loss=2.452153 time: 93.758017\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 8] train=0.370149 loss=2.354095 time: 93.808609\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 9] train=0.394362 loss=2.231290 time: 93.504651\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 10] train=0.413267 loss=2.139301 time: 93.728905\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 11] train=0.435158 loss=2.063229 time: 93.829712\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 12] train=0.458043 loss=1.978514 time: 93.768762\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 13] train=0.462023 loss=1.948867 time: 93.630444\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 14] train=0.492869 loss=1.863044 time: 93.737092\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 15] train=0.498507 loss=1.827238 time: 93.860852\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 16] train=0.511111 loss=1.740352 time: 93.558765\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 17] train=0.536318 loss=1.660067 time: 93.360708\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 18] train=0.546269 loss=1.643904 time: 93.317689\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 19] train=0.550912 loss=1.600460 time: 93.795782\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 20] train=0.565506 loss=1.567682 time: 93.809993\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 21] train=0.568823 loss=1.521534 time: 93.503070\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 22] train=0.572471 loss=1.499909 time: 93.288976\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 23] train=0.578109 loss=1.483523 time: 93.451291\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 24] train=0.608955 loss=1.431461 time: 93.548550\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 25] train=0.611609 loss=1.373609 time: 93.357038\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 26] train=0.622222 loss=1.329628 time: 93.529174\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 27] train=0.644113 loss=1.271496 time: 93.672515\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 28] train=0.641459 loss=1.259226 time: 93.676128\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 29] train=0.631177 loss=1.290659 time: 93.877639\n",
      "New learning rate:  0.001\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 30] train=0.687231 loss=1.091959 time: 93.577297\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 31] train=0.715091 loss=0.965424 time: 93.435937\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 32] train=0.720066 loss=0.948806 time: 93.768925\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 33] train=0.744610 loss=0.880132 time: 93.621413\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 34] train=0.749585 loss=0.855952 time: 93.844879\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 35] train=0.751244 loss=0.838659 time: 93.793673\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 36] train=0.766169 loss=0.785232 time: 93.438379\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 37] train=0.749585 loss=0.816193 time: 93.811097\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 38] train=0.782090 loss=0.756044 time: 93.612663\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 39] train=0.765837 loss=0.786986 time: 93.360232\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 40] train=0.785075 loss=0.747676 time: 93.496063\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 41] train=0.778109 loss=0.743504 time: 93.844919\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 42] train=0.779768 loss=0.731832 time: 93.731951\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 43] train=0.787065 loss=0.715910 time: 93.467033\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 44] train=0.799337 loss=0.697853 time: 93.273015\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 45] train=0.780431 loss=0.720792 time: 93.293849\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 46] train=0.789386 loss=0.695544 time: 93.731818\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 47] train=0.793698 loss=0.684118 time: 93.537163\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 48] train=0.804312 loss=0.645512 time: 93.462056\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 49] train=0.810282 loss=0.645273 time: 93.852008\n",
      "New learning rate:  0.0001\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 50] train=0.813599 loss=0.631583 time: 93.642432\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 51] train=0.804975 loss=0.629474 time: 93.446259\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 52] train=0.808955 loss=0.612496 time: 93.270617\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 53] train=0.825207 loss=0.583616 time: 93.288785\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 54] train=0.817910 loss=0.621123 time: 93.628845\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 55] train=0.817910 loss=0.598158 time: 93.614732\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 56] train=0.815920 loss=0.623960 time: 93.505285\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 57] train=0.811609 loss=0.629010 time: 93.568048\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 58] train=0.818905 loss=0.596542 time: 93.779252\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 59] train=0.832836 loss=0.591746 time: 93.525259\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 60] train=0.818574 loss=0.610234 time: 93.388658\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 61] train=0.816915 loss=0.615515 time: 93.583853\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 62] train=0.821891 loss=0.602654 time: 93.626723\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 63] train=0.825871 loss=0.591794 time: 93.402384\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 64] train=0.823881 loss=0.577880 time: 93.667231\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 65] train=0.817910 loss=0.591742 time: 93.808959\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 66] train=0.825207 loss=0.588407 time: 93.853692\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 67] train=0.825207 loss=0.575802 time: 93.569607\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 68] train=0.815589 loss=0.609070 time: 93.459811\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 69] train=0.819569 loss=0.579942 time: 93.776883\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 70\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        new_lr = trainer.learning_rate*lr_decay\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        print(\"New learning rate: \", new_lr)\n",
    "        lr_decay_count += 1\n",
    "        \n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        print(\".\", end=\"\")\n",
    "        # Extract data and label\n",
    "        data = split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = []\n",
    "            for _, X in enumerate(data):\n",
    "                X = X.reshape((-1,) + X.shape[2:])\n",
    "                pred = net(X)\n",
    "                output.append(pred)\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.mean().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "        if i == 200:\n",
    "            print(\"|\")\n",
    "            break\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([acc])\n",
    "    print('[Epoch %d] train=%f loss=%f time: %f' %\n",
    "        (epoch, acc, train_loss / (i+1), time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "train_history.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can `Start Training Now`_.\n",
    "\n",
    "If you would like to use a bigger 3D model (e.g., I3D) on a larger dataset (e.g., Kinetics400),\n",
    "feel free to read the next `tutorial on Kinetics400 <demo_i3d_kinetics400.html>`__.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. [Wang15] Limin Wang, Yuanjun Xiong, Zhe Wang, and Yu Qiao. \\\n",
    "    \"Towards Good Practices for Very Deep Two-Stream ConvNets.\" \\\n",
    "    arXiv preprint arXiv:1507.02159 (2015).\n",
    "\n",
    ".. [Wang16] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang and Luc Van Gool. \\\n",
    "    \"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition.\" \\\n",
    "    In European Conference on Computer Vision (ECCV). 2016.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
