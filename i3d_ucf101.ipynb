{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is copied from\n",
    "\n",
    "https://gluon-cv.mxnet.io/build/examples_action_recognition/dive_deep_i3d_kinetics400.html\n",
    "\n",
    "and modified to be used on the UCF101 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Dive Deep into Training I3D mdoels on UCF101\n",
    "=======================================================\n",
    "\n",
    "This is a video action recognition tutorial using Gluon CV toolkit, a step-by-step example.\n",
    "The readers should have basic knowledge of deep learning and should be familiar with Gluon API.\n",
    "New users may first go through `A 60-minute Gluon Crash Course <http://gluon-crash-course.mxnet.io/>`_.\n",
    "You can `Start Training Now`_ or `Dive into Deep`_.\n",
    "\n",
    "Start Training Now\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "Network Structure\n",
    "-----------------\n",
    "\n",
    "First, let's import the necessary libraries into python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, os, sys, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluoncv as gcv\n",
    "from mxnet import gluon, nd, init, context\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.data.transforms import video\n",
    "from gluoncv.data import UCF101\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, LRSequential, LRScheduler, split_and_load, TrainingHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pick a widely adopted model, ``I3D-InceptionV1``, for the tutorial.\n",
    "`I3D <https://arxiv.org/abs/1705.07750>`_ (Inflated 3D Networks) is a widely adopted 3D video\n",
    "classification network. It uses 3D convolution to learn spatiotemporal information directly from videos.\n",
    "I3D is proposed to improve C3D model by inflating from 2D models.\n",
    "We can not only reuse the 2D models' architecture (e.g., ResNet, Inception), but also bootstrap\n",
    "the model weights from 2D pretrained models. In this manner, training 3D networks for video\n",
    "classification is feasible and getting much better results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0_weight is done with shape:  (64, 3, 5, 7, 7)\n",
      "batchnorm0_gamma is done with shape:  (64,)\n",
      "batchnorm0_beta is done with shape:  (64,)\n",
      "batchnorm0_running_mean is done with shape:  (64,)\n",
      "batchnorm0_running_var is done with shape:  (64,)\n",
      "layer1_0_conv0_weight is done with shape:  (64, 64, 3, 1, 1)\n",
      "layer1_0_batchnorm0_gamma is done with shape:  (64,)\n",
      "layer1_0_batchnorm0_beta is done with shape:  (64,)\n",
      "layer1_0_batchnorm0_running_mean is done with shape:  (64,)\n",
      "layer1_0_batchnorm0_running_var is done with shape:  (64,)\n",
      "layer1_0_conv1_weight is done with shape:  (64, 64, 1, 3, 3)\n",
      "layer1_0_batchnorm1_gamma is done with shape:  (64,)\n",
      "layer1_0_batchnorm1_beta is done with shape:  (64,)\n",
      "layer1_0_batchnorm1_running_mean is done with shape:  (64,)\n",
      "layer1_0_batchnorm1_running_var is done with shape:  (64,)\n",
      "layer1_0_conv2_weight is done with shape:  (256, 64, 1, 1, 1)\n",
      "layer1_0_batchnorm2_gamma is done with shape:  (256,)\n",
      "layer1_0_batchnorm2_beta is done with shape:  (256,)\n",
      "layer1_0_batchnorm2_running_mean is done with shape:  (256,)\n",
      "layer1_0_batchnorm2_running_var is done with shape:  (256,)\n",
      "layer1_downsample_conv0_weight is done with shape:  (256, 64, 1, 1, 1)\n",
      "layer1_downsample_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer1_downsample_batchnorm0_beta is done with shape:  (256,)\n",
      "layer1_downsample_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer1_downsample_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer1_1_conv0_weight is done with shape:  (64, 256, 3, 1, 1)\n",
      "layer1_1_batchnorm0_gamma is done with shape:  (64,)\n",
      "layer1_1_batchnorm0_beta is done with shape:  (64,)\n",
      "layer1_1_batchnorm0_running_mean is done with shape:  (64,)\n",
      "layer1_1_batchnorm0_running_var is done with shape:  (64,)\n",
      "layer1_1_conv1_weight is done with shape:  (64, 64, 1, 3, 3)\n",
      "layer1_1_batchnorm1_gamma is done with shape:  (64,)\n",
      "layer1_1_batchnorm1_beta is done with shape:  (64,)\n",
      "layer1_1_batchnorm1_running_mean is done with shape:  (64,)\n",
      "layer1_1_batchnorm1_running_var is done with shape:  (64,)\n",
      "layer1_1_conv2_weight is done with shape:  (256, 64, 1, 1, 1)\n",
      "layer1_1_batchnorm2_gamma is done with shape:  (256,)\n",
      "layer1_1_batchnorm2_beta is done with shape:  (256,)\n",
      "layer1_1_batchnorm2_running_mean is done with shape:  (256,)\n",
      "layer1_1_batchnorm2_running_var is done with shape:  (256,)\n",
      "layer1_2_conv0_weight is done with shape:  (64, 256, 3, 1, 1)\n",
      "layer1_2_batchnorm0_gamma is done with shape:  (64,)\n",
      "layer1_2_batchnorm0_beta is done with shape:  (64,)\n",
      "layer1_2_batchnorm0_running_mean is done with shape:  (64,)\n",
      "layer1_2_batchnorm0_running_var is done with shape:  (64,)\n",
      "layer1_2_conv1_weight is done with shape:  (64, 64, 1, 3, 3)\n",
      "layer1_2_batchnorm1_gamma is done with shape:  (64,)\n",
      "layer1_2_batchnorm1_beta is done with shape:  (64,)\n",
      "layer1_2_batchnorm1_running_mean is done with shape:  (64,)\n",
      "layer1_2_batchnorm1_running_var is done with shape:  (64,)\n",
      "layer1_2_conv2_weight is done with shape:  (256, 64, 1, 1, 1)\n",
      "layer1_2_batchnorm2_gamma is done with shape:  (256,)\n",
      "layer1_2_batchnorm2_beta is done with shape:  (256,)\n",
      "layer1_2_batchnorm2_running_mean is done with shape:  (256,)\n",
      "layer1_2_batchnorm2_running_var is done with shape:  (256,)\n",
      "layer2_0_conv0_weight is done with shape:  (128, 256, 3, 1, 1)\n",
      "layer2_0_batchnorm0_gamma is done with shape:  (128,)\n",
      "layer2_0_batchnorm0_beta is done with shape:  (128,)\n",
      "layer2_0_batchnorm0_running_mean is done with shape:  (128,)\n",
      "layer2_0_batchnorm0_running_var is done with shape:  (128,)\n",
      "layer2_0_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
      "layer2_0_batchnorm1_gamma is done with shape:  (128,)\n",
      "layer2_0_batchnorm1_beta is done with shape:  (128,)\n",
      "layer2_0_batchnorm1_running_mean is done with shape:  (128,)\n",
      "layer2_0_batchnorm1_running_var is done with shape:  (128,)\n",
      "layer2_0_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
      "layer2_0_batchnorm2_gamma is done with shape:  (512,)\n",
      "layer2_0_batchnorm2_beta is done with shape:  (512,)\n",
      "layer2_0_batchnorm2_running_mean is done with shape:  (512,)\n",
      "layer2_0_batchnorm2_running_var is done with shape:  (512,)\n",
      "layer2_downsample_conv0_weight is done with shape:  (512, 256, 1, 1, 1)\n",
      "layer2_downsample_batchnorm0_gamma is done with shape:  (512,)\n",
      "layer2_downsample_batchnorm0_beta is done with shape:  (512,)\n",
      "layer2_downsample_batchnorm0_running_mean is done with shape:  (512,)\n",
      "layer2_downsample_batchnorm0_running_var is done with shape:  (512,)\n",
      "layer2_1_conv0_weight is done with shape:  (128, 512, 1, 1, 1)\n",
      "layer2_1_batchnorm0_gamma is done with shape:  (128,)\n",
      "layer2_1_batchnorm0_beta is done with shape:  (128,)\n",
      "layer2_1_batchnorm0_running_mean is done with shape:  (128,)\n",
      "layer2_1_batchnorm0_running_var is done with shape:  (128,)\n",
      "layer2_1_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
      "layer2_1_batchnorm1_gamma is done with shape:  (128,)\n",
      "layer2_1_batchnorm1_beta is done with shape:  (128,)\n",
      "layer2_1_batchnorm1_running_mean is done with shape:  (128,)\n",
      "layer2_1_batchnorm1_running_var is done with shape:  (128,)\n",
      "layer2_1_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
      "layer2_1_batchnorm2_gamma is done with shape:  (512,)\n",
      "layer2_1_batchnorm2_beta is done with shape:  (512,)\n",
      "layer2_1_batchnorm2_running_mean is done with shape:  (512,)\n",
      "layer2_1_batchnorm2_running_var is done with shape:  (512,)\n",
      "layer2_2_conv0_weight is done with shape:  (128, 512, 3, 1, 1)\n",
      "layer2_2_batchnorm0_gamma is done with shape:  (128,)\n",
      "layer2_2_batchnorm0_beta is done with shape:  (128,)\n",
      "layer2_2_batchnorm0_running_mean is done with shape:  (128,)\n",
      "layer2_2_batchnorm0_running_var is done with shape:  (128,)\n",
      "layer2_2_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
      "layer2_2_batchnorm1_gamma is done with shape:  (128,)\n",
      "layer2_2_batchnorm1_beta is done with shape:  (128,)\n",
      "layer2_2_batchnorm1_running_mean is done with shape:  (128,)\n",
      "layer2_2_batchnorm1_running_var is done with shape:  (128,)\n",
      "layer2_2_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
      "layer2_2_batchnorm2_gamma is done with shape:  (512,)\n",
      "layer2_2_batchnorm2_beta is done with shape:  (512,)\n",
      "layer2_2_batchnorm2_running_mean is done with shape:  (512,)\n",
      "layer2_2_batchnorm2_running_var is done with shape:  (512,)\n",
      "layer2_3_conv0_weight is done with shape:  (128, 512, 1, 1, 1)\n",
      "layer2_3_batchnorm0_gamma is done with shape:  (128,)\n",
      "layer2_3_batchnorm0_beta is done with shape:  (128,)\n",
      "layer2_3_batchnorm0_running_mean is done with shape:  (128,)\n",
      "layer2_3_batchnorm0_running_var is done with shape:  (128,)\n",
      "layer2_3_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
      "layer2_3_batchnorm1_gamma is done with shape:  (128,)\n",
      "layer2_3_batchnorm1_beta is done with shape:  (128,)\n",
      "layer2_3_batchnorm1_running_mean is done with shape:  (128,)\n",
      "layer2_3_batchnorm1_running_var is done with shape:  (128,)\n",
      "layer2_3_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
      "layer2_3_batchnorm2_gamma is done with shape:  (512,)\n",
      "layer2_3_batchnorm2_beta is done with shape:  (512,)\n",
      "layer2_3_batchnorm2_running_mean is done with shape:  (512,)\n",
      "layer2_3_batchnorm2_running_var is done with shape:  (512,)\n",
      "layer3_0_conv0_weight is done with shape:  (256, 512, 3, 1, 1)\n",
      "layer3_0_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_0_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_0_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_0_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_0_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_0_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_0_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_0_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_0_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_0_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_0_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_0_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_0_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_0_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer3_downsample_conv0_weight is done with shape:  (1024, 512, 1, 1, 1)\n",
      "layer3_downsample_batchnorm0_gamma is done with shape:  (1024,)\n",
      "layer3_downsample_batchnorm0_beta is done with shape:  (1024,)\n",
      "layer3_downsample_batchnorm0_running_mean is done with shape:  (1024,)\n",
      "layer3_downsample_batchnorm0_running_var is done with shape:  (1024,)\n",
      "layer3_1_conv0_weight is done with shape:  (256, 1024, 1, 1, 1)\n",
      "layer3_1_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_1_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_1_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_1_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_1_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_1_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_1_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_1_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_1_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_1_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_1_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_1_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_1_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_1_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer3_2_conv0_weight is done with shape:  (256, 1024, 3, 1, 1)\n",
      "layer3_2_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_2_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_2_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_2_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_2_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_2_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_2_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_2_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_2_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_2_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_2_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_2_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_2_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_2_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer3_3_conv0_weight is done with shape:  (256, 1024, 1, 1, 1)\n",
      "layer3_3_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_3_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_3_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_3_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_3_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_3_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_3_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_3_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_3_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_3_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_3_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_3_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_3_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_3_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer3_4_conv0_weight is done with shape:  (256, 1024, 3, 1, 1)\n",
      "layer3_4_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_4_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_4_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_4_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_4_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_4_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_4_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_4_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_4_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_4_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_4_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_4_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_4_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_4_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer3_5_conv0_weight is done with shape:  (256, 1024, 1, 1, 1)\n",
      "layer3_5_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_5_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_5_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_5_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_5_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_5_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_5_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_5_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_5_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_5_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_5_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_5_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_5_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_5_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer4_0_conv0_weight is done with shape:  (512, 1024, 1, 1, 1)\n",
      "layer4_0_batchnorm0_gamma is done with shape:  (512,)\n",
      "layer4_0_batchnorm0_beta is done with shape:  (512,)\n",
      "layer4_0_batchnorm0_running_mean is done with shape:  (512,)\n",
      "layer4_0_batchnorm0_running_var is done with shape:  (512,)\n",
      "layer4_0_conv1_weight is done with shape:  (512, 512, 1, 3, 3)\n",
      "layer4_0_batchnorm1_gamma is done with shape:  (512,)\n",
      "layer4_0_batchnorm1_beta is done with shape:  (512,)\n",
      "layer4_0_batchnorm1_running_mean is done with shape:  (512,)\n",
      "layer4_0_batchnorm1_running_var is done with shape:  (512,)\n",
      "layer4_0_conv2_weight is done with shape:  (2048, 512, 1, 1, 1)\n",
      "layer4_0_batchnorm2_gamma is done with shape:  (2048,)\n",
      "layer4_0_batchnorm2_beta is done with shape:  (2048,)\n",
      "layer4_0_batchnorm2_running_mean is done with shape:  (2048,)\n",
      "layer4_0_batchnorm2_running_var is done with shape:  (2048,)\n",
      "layer4_downsample_conv0_weight is done with shape:  (2048, 1024, 1, 1, 1)\n",
      "layer4_downsample_batchnorm0_gamma is done with shape:  (2048,)\n",
      "layer4_downsample_batchnorm0_beta is done with shape:  (2048,)\n",
      "layer4_downsample_batchnorm0_running_mean is done with shape:  (2048,)\n",
      "layer4_downsample_batchnorm0_running_var is done with shape:  (2048,)\n",
      "layer4_1_conv0_weight is done with shape:  (512, 2048, 3, 1, 1)\n",
      "layer4_1_batchnorm0_gamma is done with shape:  (512,)\n",
      "layer4_1_batchnorm0_beta is done with shape:  (512,)\n",
      "layer4_1_batchnorm0_running_mean is done with shape:  (512,)\n",
      "layer4_1_batchnorm0_running_var is done with shape:  (512,)\n",
      "layer4_1_conv1_weight is done with shape:  (512, 512, 1, 3, 3)\n",
      "layer4_1_batchnorm1_gamma is done with shape:  (512,)\n",
      "layer4_1_batchnorm1_beta is done with shape:  (512,)\n",
      "layer4_1_batchnorm1_running_mean is done with shape:  (512,)\n",
      "layer4_1_batchnorm1_running_var is done with shape:  (512,)\n",
      "layer4_1_conv2_weight is done with shape:  (2048, 512, 1, 1, 1)\n",
      "layer4_1_batchnorm2_gamma is done with shape:  (2048,)\n",
      "layer4_1_batchnorm2_beta is done with shape:  (2048,)\n",
      "layer4_1_batchnorm2_running_mean is done with shape:  (2048,)\n",
      "layer4_1_batchnorm2_running_var is done with shape:  (2048,)\n",
      "layer4_2_conv0_weight is done with shape:  (512, 2048, 1, 1, 1)\n",
      "layer4_2_batchnorm0_gamma is done with shape:  (512,)\n",
      "layer4_2_batchnorm0_beta is done with shape:  (512,)\n",
      "layer4_2_batchnorm0_running_mean is done with shape:  (512,)\n",
      "layer4_2_batchnorm0_running_var is done with shape:  (512,)\n",
      "layer4_2_conv1_weight is done with shape:  (512, 512, 1, 3, 3)\n",
      "layer4_2_batchnorm1_gamma is done with shape:  (512,)\n",
      "layer4_2_batchnorm1_beta is done with shape:  (512,)\n",
      "layer4_2_batchnorm1_running_mean is done with shape:  (512,)\n",
      "layer4_2_batchnorm1_running_var is done with shape:  (512,)\n",
      "layer4_2_conv2_weight is done with shape:  (2048, 512, 1, 1, 1)\n",
      "layer4_2_batchnorm2_gamma is done with shape:  (2048,)\n",
      "layer4_2_batchnorm2_beta is done with shape:  (2048,)\n",
      "layer4_2_batchnorm2_running_mean is done with shape:  (2048,)\n",
      "layer4_2_batchnorm2_running_var is done with shape:  (2048,)\n",
      "dense0_weight is skipped with shape:  (101, 2048)\n",
      "dense0_bias is skipped with shape:  (101,)\n",
      "I3D_ResNetV1(\n",
      "  (first_stage): HybridSequential(\n",
      "    (0): Conv3D(3 -> 64, kernel_size=(5, 7, 7), stride=(2, 2, 2), padding=(2, 3, 3), bias=False)\n",
      "    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "    (2): Activation(relu)\n",
      "    (3): MaxPool3D(size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCDHW)\n",
      "  )\n",
      "  (pool2): MaxPool3D(size=(2, 1, 1), stride=(2, 1, 1), padding=(0, 0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCDHW)\n",
      "  (res_layers): HybridSequential(\n",
      "    (0): HybridSequential(\n",
      "      (0): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(64 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        )\n",
      "        (conv1): Conv3D(64 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        )\n",
      "        (conv1): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        )\n",
      "        (conv1): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "    )\n",
      "    (1): HybridSequential(\n",
      "      (0): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(256 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        )\n",
      "        (conv1): Conv3D(256 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv3D(256 -> 512, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        )\n",
      "        (conv1): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(512 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        )\n",
      "        (conv1): Conv3D(512 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        )\n",
      "        (conv1): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "    )\n",
      "    (2): HybridSequential(\n",
      "      (0): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(512 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(512 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv3D(512 -> 1024, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=1024)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "    )\n",
      "    (3): HybridSequential(\n",
      "      (0): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        (relu): Activation(relu)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 2048, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=2048)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(2048 -> 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        )\n",
      "        (conv1): Conv3D(2048 -> 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(2048 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        )\n",
      "        (conv1): Conv3D(2048 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (st_avg): GlobalAvgPool3D(size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCDHW)\n",
      "  (head): HybridSequential(\n",
      "    (0): Dropout(p = 0.8, axes=())\n",
      "    (1): Dense(2048 -> 101, linear)\n",
      "  )\n",
      "  (fc): Dense(2048 -> 101, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# Get the model i3d_resnet50_v1_ucf101 with 101 output classes, without pre-trained weights\n",
    "net = get_model(name='i3d_resnet50_v1_ucf101', nclass=101)\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation and Data Loader\n",
    "---------------------------------\n",
    "\n",
    "Data augmentation for video is different from image. For example, if you\n",
    "want to randomly crop a video sequence, you need to make sure all the video\n",
    "frames in this sequence undergo the same cropping process. We provide a\n",
    "new set of transformation functions, working with multiple images.\n",
    "Please checkout the `video.py <../../../gluoncv/data/transforms/video.py>`_ for more details.\n",
    "Most video data augmentation strategies used here are introduced in [Wang15]_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # Fix the input video frames size as 256340 and randomly sample the cropping width and height from\n",
    "    # {256,224,192,168}. After that, resize the cropped regions to 224  224.\n",
    "    video.VideoMultiScaleCrop(size=(224, 224), scale_ratios=[1.0, 0.875, 0.75, 0.66]),\n",
    "    # Randomly flip the video frames horizontally\n",
    "    video.VideoRandomHorizontalFlip(),\n",
    "    # Transpose the video frames from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    video.VideoToTensor(),\n",
    "    # Normalize the video frames with mean and standard deviation calculated across all images\n",
    "    video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the transform functions, we can define data loaders for our\n",
    "training datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 9537 training samples.\n"
     ]
    }
   ],
   "source": [
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 5\n",
    "# Number of data loader workers\n",
    "num_workers = 1\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training the model.\n",
    "# ``new_length`` indicates the number of frames we use as input.\n",
    "# ``new_step`` indicates we skip one frame to sample the input data.\n",
    "train_dataset = UCF101(train=True, new_length=32, new_step=2, transform=transform_train)\n",
    "print('Load %d training samples.' % len(train_dataset))\n",
    "train_data = gluon.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                   shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, Loss and Metric\n",
    "--------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [40, 70, 90, np.inf]\n",
    "\n",
    "# Stochastic gradient descent\n",
    "optimizer = 'sgd'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.01, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize our model, we need a loss function.\n",
    "For classification tasks, we usually use softmax cross entropy as the\n",
    "loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we use accuracy as the metric to monitor our training\n",
    "process. Besides, we record metric values, and will print them at the\n",
    "end of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "After all the preparations, we can finally start training!\n",
    "Following is the script.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In order to finish the tutorial quickly, we only train for 3 epochs on a tiny subset of UCF101,\n",
    "  and 100 iterations per epoch. In your experiments, we recommend setting ``epochs=100`` for the full UCF101 dataset.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..|\n",
      "[Epoch 0] train=0.033333 loss=4.621305 time: 35.221419\n",
      "..|\n",
      "[Epoch 1] train=0.000000 loss=4.631328 time: 7.776535\n",
      "..|\n",
      "[Epoch 2] train=0.133333 loss=4.556106 time: 10.027771\n",
      "..|\n",
      "[Epoch 3] train=0.066667 loss=4.492631 time: 10.099543\n",
      "..|\n",
      "[Epoch 4] train=0.033333 loss=4.553545 time: 10.160951\n",
      "..|\n",
      "[Epoch 5] train=0.100000 loss=4.485622 time: 10.116610\n",
      "..|\n",
      "[Epoch 6] train=0.133333 loss=4.430291 time: 10.406407\n",
      "..|\n",
      "[Epoch 7] train=0.200000 loss=3.942653 time: 10.180080\n",
      "..|\n",
      "[Epoch 8] train=0.033333 loss=4.304532 time: 10.247751\n",
      "..|\n",
      "[Epoch 9] train=0.166667 loss=4.162950 time: 10.330554\n",
      "..|\n",
      "[Epoch 10] train=0.166667 loss=4.184868 time: 10.081864\n",
      "..|\n",
      "[Epoch 11] train=0.266667 loss=3.957918 time: 9.921786\n",
      "..|\n",
      "[Epoch 12] train=0.266667 loss=3.627400 time: 10.099184\n",
      "..|\n",
      "[Epoch 13] train=0.166667 loss=3.834834 time: 10.266018\n",
      "..|\n",
      "[Epoch 14] train=0.366667 loss=3.501137 time: 10.132319\n",
      "..|\n",
      "[Epoch 15] train=0.200000 loss=3.760755 time: 9.878500\n",
      "..|\n",
      "[Epoch 16] train=0.333333 loss=3.346661 time: 10.489181\n",
      "..|\n",
      "[Epoch 17] train=0.333333 loss=3.041375 time: 10.324908\n",
      "..|\n",
      "[Epoch 18] train=0.333333 loss=3.201984 time: 10.089200\n",
      "..|\n",
      "[Epoch 19] train=0.233333 loss=3.313737 time: 10.167247\n",
      "..|\n",
      "[Epoch 20] train=0.266667 loss=3.504995 time: 10.325363\n",
      "..|\n",
      "[Epoch 21] train=0.333333 loss=3.077824 time: 10.196736\n",
      "..|\n",
      "[Epoch 22] train=0.566667 loss=2.375479 time: 10.118370\n",
      "..|\n",
      "[Epoch 23] train=0.333333 loss=2.943877 time: 10.162396\n",
      "..|\n",
      "[Epoch 24] train=0.400000 loss=2.998907 time: 9.980892\n",
      "..|\n",
      "[Epoch 25] train=0.500000 loss=2.831205 time: 9.919064\n",
      "..|\n",
      "[Epoch 26] train=0.500000 loss=2.648675 time: 10.359044\n",
      "..|\n",
      "[Epoch 27] train=0.300000 loss=2.590805 time: 11.243896\n",
      "..|\n",
      "[Epoch 28] train=0.433333 loss=2.387364 time: 10.144599\n",
      "..|\n",
      "[Epoch 29] train=0.500000 loss=2.406498 time: 10.765889\n",
      "..|\n",
      "[Epoch 30] train=0.433333 loss=2.479204 time: 9.878547\n",
      "..|\n",
      "[Epoch 31] train=0.566667 loss=2.046043 time: 9.494684\n",
      "..|\n",
      "[Epoch 32] train=0.433333 loss=2.396575 time: 9.661050\n",
      "..|\n",
      "[Epoch 33] train=0.366667 loss=2.353078 time: 9.561019\n",
      "..|\n",
      "[Epoch 34] train=0.466667 loss=2.245525 time: 9.862617\n",
      "..|\n",
      "[Epoch 35] train=0.466667 loss=2.117912 time: 9.898244\n",
      "..|\n",
      "[Epoch 36] train=0.633333 loss=1.634253 time: 9.863604\n",
      "..|\n",
      "[Epoch 37] train=0.466667 loss=2.178868 time: 9.618841\n",
      "..|\n",
      "[Epoch 38] train=0.466667 loss=2.051404 time: 9.778232\n",
      "..|\n",
      "[Epoch 39] train=0.433333 loss=2.315474 time: 9.335348\n",
      "New learning rate:  0.001\n",
      "..|\n",
      "[Epoch 40] train=0.633333 loss=1.611969 time: 9.142853\n",
      "..|\n",
      "[Epoch 41] train=0.566667 loss=1.682851 time: 9.566684\n",
      "..|\n",
      "[Epoch 42] train=0.500000 loss=2.270549 time: 9.514223\n",
      "..|\n",
      "[Epoch 43] train=0.400000 loss=2.263498 time: 9.451697\n",
      "..|\n",
      "[Epoch 44] train=0.233333 loss=2.787373 time: 9.425617\n",
      "..|\n",
      "[Epoch 45] train=0.366667 loss=2.222807 time: 9.369181\n",
      "..|\n",
      "[Epoch 46] train=0.300000 loss=2.751752 time: 9.559425\n",
      "..|\n",
      "[Epoch 47] train=0.366667 loss=2.364449 time: 9.556699\n",
      "..|\n",
      "[Epoch 48] train=0.333333 loss=2.374283 time: 9.411819\n",
      "..|\n",
      "[Epoch 49] train=0.433333 loss=2.556181 time: 9.380000\n",
      "..|\n",
      "[Epoch 50] train=0.533333 loss=2.175743 time: 9.257033\n",
      "..|\n",
      "[Epoch 51] train=0.533333 loss=1.622691 time: 9.379555\n",
      "..|\n",
      "[Epoch 52] train=0.400000 loss=2.606800 time: 9.361887\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        new_lr = trainer.learning_rate*lr_decay\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        print(\"New learning rate: \", new_lr)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        print(\".\", end=\"\")\n",
    "        # Extract data and label\n",
    "        data = split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = []\n",
    "            for _, X in enumerate(data):\n",
    "                X = X.reshape((-1,) + X.shape[2:])\n",
    "                pred = net(X)\n",
    "                output.append(pred)\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.mean().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "        if i == 1:\n",
    "            print(\"|\")\n",
    "            break\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([acc])\n",
    "    print('[Epoch %d] train=%f loss=%f time: %f' %\n",
    "        (epoch, acc, train_loss / (i+1), time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "train_history.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. Joao Carreira and Andrew Zisserman. Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. In Computer Vision and Pattern Recognition (CVPR), 2017.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
