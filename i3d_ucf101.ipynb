{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is copied from\n",
    "\n",
    "https://gluon-cv.mxnet.io/build/examples_action_recognition/dive_deep_i3d_kinetics400.html\n",
    "\n",
    "and modified to be used on the UCF101 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Dive Deep into Training I3D mdoels on UCF101\n",
    "=======================================================\n",
    "\n",
    "This is a video action recognition tutorial using Gluon CV toolkit, a step-by-step example.\n",
    "The readers should have basic knowledge of deep learning and should be familiar with Gluon API.\n",
    "New users may first go through `A 60-minute Gluon Crash Course <http://gluon-crash-course.mxnet.io/>`_.\n",
    "You can `Start Training Now`_ or `Dive into Deep`_.\n",
    "\n",
    "Start Training Now\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "Network Structure\n",
    "-----------------\n",
    "\n",
    "First, let's import the necessary libraries into python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, os, sys, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluoncv as gcv\n",
    "from mxnet import gluon, nd, init, context\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.data.transforms import video\n",
    "from gluoncv.data import UCF101\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, LRSequential, LRScheduler, split_and_load, TrainingHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pick a widely adopted model, ``I3D-InceptionV1``, for the tutorial.\n",
    "`I3D <https://arxiv.org/abs/1705.07750>`_ (Inflated 3D Networks) is a widely adopted 3D video\n",
    "classification network. It uses 3D convolution to learn spatiotemporal information directly from videos.\n",
    "I3D is proposed to improve C3D model by inflating from 2D models.\n",
    "We can not only reuse the 2D models' architecture (e.g., ResNet, Inception), but also bootstrap\n",
    "the model weights from 2D pretrained models. In this manner, training 3D networks for video\n",
    "classification is feasible and getting much better results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0_weight is done with shape:  (64, 3, 5, 7, 7)\n",
      "batchnorm0_gamma is done with shape:  (64,)\n",
      "batchnorm0_beta is done with shape:  (64,)\n",
      "batchnorm0_running_mean is done with shape:  (64,)\n",
      "batchnorm0_running_var is done with shape:  (64,)\n",
      "layer1_0_conv0_weight is done with shape:  (64, 64, 3, 1, 1)\n",
      "layer1_0_batchnorm0_gamma is done with shape:  (64,)\n",
      "layer1_0_batchnorm0_beta is done with shape:  (64,)\n",
      "layer1_0_batchnorm0_running_mean is done with shape:  (64,)\n",
      "layer1_0_batchnorm0_running_var is done with shape:  (64,)\n",
      "layer1_0_conv1_weight is done with shape:  (64, 64, 1, 3, 3)\n",
      "layer1_0_batchnorm1_gamma is done with shape:  (64,)\n",
      "layer1_0_batchnorm1_beta is done with shape:  (64,)\n",
      "layer1_0_batchnorm1_running_mean is done with shape:  (64,)\n",
      "layer1_0_batchnorm1_running_var is done with shape:  (64,)\n",
      "layer1_0_conv2_weight is done with shape:  (256, 64, 1, 1, 1)\n",
      "layer1_0_batchnorm2_gamma is done with shape:  (256,)\n",
      "layer1_0_batchnorm2_beta is done with shape:  (256,)\n",
      "layer1_0_batchnorm2_running_mean is done with shape:  (256,)\n",
      "layer1_0_batchnorm2_running_var is done with shape:  (256,)\n",
      "layer1_downsample_conv0_weight is done with shape:  (256, 64, 1, 1, 1)\n",
      "layer1_downsample_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer1_downsample_batchnorm0_beta is done with shape:  (256,)\n",
      "layer1_downsample_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer1_downsample_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer1_1_conv0_weight is done with shape:  (64, 256, 3, 1, 1)\n",
      "layer1_1_batchnorm0_gamma is done with shape:  (64,)\n",
      "layer1_1_batchnorm0_beta is done with shape:  (64,)\n",
      "layer1_1_batchnorm0_running_mean is done with shape:  (64,)\n",
      "layer1_1_batchnorm0_running_var is done with shape:  (64,)\n",
      "layer1_1_conv1_weight is done with shape:  (64, 64, 1, 3, 3)\n",
      "layer1_1_batchnorm1_gamma is done with shape:  (64,)\n",
      "layer1_1_batchnorm1_beta is done with shape:  (64,)\n",
      "layer1_1_batchnorm1_running_mean is done with shape:  (64,)\n",
      "layer1_1_batchnorm1_running_var is done with shape:  (64,)\n",
      "layer1_1_conv2_weight is done with shape:  (256, 64, 1, 1, 1)\n",
      "layer1_1_batchnorm2_gamma is done with shape:  (256,)\n",
      "layer1_1_batchnorm2_beta is done with shape:  (256,)\n",
      "layer1_1_batchnorm2_running_mean is done with shape:  (256,)\n",
      "layer1_1_batchnorm2_running_var is done with shape:  (256,)\n",
      "layer1_2_conv0_weight is done with shape:  (64, 256, 3, 1, 1)\n",
      "layer1_2_batchnorm0_gamma is done with shape:  (64,)\n",
      "layer1_2_batchnorm0_beta is done with shape:  (64,)\n",
      "layer1_2_batchnorm0_running_mean is done with shape:  (64,)\n",
      "layer1_2_batchnorm0_running_var is done with shape:  (64,)\n",
      "layer1_2_conv1_weight is done with shape:  (64, 64, 1, 3, 3)\n",
      "layer1_2_batchnorm1_gamma is done with shape:  (64,)\n",
      "layer1_2_batchnorm1_beta is done with shape:  (64,)\n",
      "layer1_2_batchnorm1_running_mean is done with shape:  (64,)\n",
      "layer1_2_batchnorm1_running_var is done with shape:  (64,)\n",
      "layer1_2_conv2_weight is done with shape:  (256, 64, 1, 1, 1)\n",
      "layer1_2_batchnorm2_gamma is done with shape:  (256,)\n",
      "layer1_2_batchnorm2_beta is done with shape:  (256,)\n",
      "layer1_2_batchnorm2_running_mean is done with shape:  (256,)\n",
      "layer1_2_batchnorm2_running_var is done with shape:  (256,)\n",
      "layer2_0_conv0_weight is done with shape:  (128, 256, 3, 1, 1)\n",
      "layer2_0_batchnorm0_gamma is done with shape:  (128,)\n",
      "layer2_0_batchnorm0_beta is done with shape:  (128,)\n",
      "layer2_0_batchnorm0_running_mean is done with shape:  (128,)\n",
      "layer2_0_batchnorm0_running_var is done with shape:  (128,)\n",
      "layer2_0_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
      "layer2_0_batchnorm1_gamma is done with shape:  (128,)\n",
      "layer2_0_batchnorm1_beta is done with shape:  (128,)\n",
      "layer2_0_batchnorm1_running_mean is done with shape:  (128,)\n",
      "layer2_0_batchnorm1_running_var is done with shape:  (128,)\n",
      "layer2_0_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
      "layer2_0_batchnorm2_gamma is done with shape:  (512,)\n",
      "layer2_0_batchnorm2_beta is done with shape:  (512,)\n",
      "layer2_0_batchnorm2_running_mean is done with shape:  (512,)\n",
      "layer2_0_batchnorm2_running_var is done with shape:  (512,)\n",
      "layer2_downsample_conv0_weight is done with shape:  (512, 256, 1, 1, 1)\n",
      "layer2_downsample_batchnorm0_gamma is done with shape:  (512,)\n",
      "layer2_downsample_batchnorm0_beta is done with shape:  (512,)\n",
      "layer2_downsample_batchnorm0_running_mean is done with shape:  (512,)\n",
      "layer2_downsample_batchnorm0_running_var is done with shape:  (512,)\n",
      "layer2_1_conv0_weight is done with shape:  (128, 512, 1, 1, 1)\n",
      "layer2_1_batchnorm0_gamma is done with shape:  (128,)\n",
      "layer2_1_batchnorm0_beta is done with shape:  (128,)\n",
      "layer2_1_batchnorm0_running_mean is done with shape:  (128,)\n",
      "layer2_1_batchnorm0_running_var is done with shape:  (128,)\n",
      "layer2_1_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
      "layer2_1_batchnorm1_gamma is done with shape:  (128,)\n",
      "layer2_1_batchnorm1_beta is done with shape:  (128,)\n",
      "layer2_1_batchnorm1_running_mean is done with shape:  (128,)\n",
      "layer2_1_batchnorm1_running_var is done with shape:  (128,)\n",
      "layer2_1_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
      "layer2_1_batchnorm2_gamma is done with shape:  (512,)\n",
      "layer2_1_batchnorm2_beta is done with shape:  (512,)\n",
      "layer2_1_batchnorm2_running_mean is done with shape:  (512,)\n",
      "layer2_1_batchnorm2_running_var is done with shape:  (512,)\n",
      "layer2_2_conv0_weight is done with shape:  (128, 512, 3, 1, 1)\n",
      "layer2_2_batchnorm0_gamma is done with shape:  (128,)\n",
      "layer2_2_batchnorm0_beta is done with shape:  (128,)\n",
      "layer2_2_batchnorm0_running_mean is done with shape:  (128,)\n",
      "layer2_2_batchnorm0_running_var is done with shape:  (128,)\n",
      "layer2_2_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
      "layer2_2_batchnorm1_gamma is done with shape:  (128,)\n",
      "layer2_2_batchnorm1_beta is done with shape:  (128,)\n",
      "layer2_2_batchnorm1_running_mean is done with shape:  (128,)\n",
      "layer2_2_batchnorm1_running_var is done with shape:  (128,)\n",
      "layer2_2_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
      "layer2_2_batchnorm2_gamma is done with shape:  (512,)\n",
      "layer2_2_batchnorm2_beta is done with shape:  (512,)\n",
      "layer2_2_batchnorm2_running_mean is done with shape:  (512,)\n",
      "layer2_2_batchnorm2_running_var is done with shape:  (512,)\n",
      "layer2_3_conv0_weight is done with shape:  (128, 512, 1, 1, 1)\n",
      "layer2_3_batchnorm0_gamma is done with shape:  (128,)\n",
      "layer2_3_batchnorm0_beta is done with shape:  (128,)\n",
      "layer2_3_batchnorm0_running_mean is done with shape:  (128,)\n",
      "layer2_3_batchnorm0_running_var is done with shape:  (128,)\n",
      "layer2_3_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
      "layer2_3_batchnorm1_gamma is done with shape:  (128,)\n",
      "layer2_3_batchnorm1_beta is done with shape:  (128,)\n",
      "layer2_3_batchnorm1_running_mean is done with shape:  (128,)\n",
      "layer2_3_batchnorm1_running_var is done with shape:  (128,)\n",
      "layer2_3_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
      "layer2_3_batchnorm2_gamma is done with shape:  (512,)\n",
      "layer2_3_batchnorm2_beta is done with shape:  (512,)\n",
      "layer2_3_batchnorm2_running_mean is done with shape:  (512,)\n",
      "layer2_3_batchnorm2_running_var is done with shape:  (512,)\n",
      "layer3_0_conv0_weight is done with shape:  (256, 512, 3, 1, 1)\n",
      "layer3_0_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_0_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_0_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_0_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_0_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_0_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_0_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_0_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_0_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_0_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_0_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_0_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_0_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_0_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer3_downsample_conv0_weight is done with shape:  (1024, 512, 1, 1, 1)\n",
      "layer3_downsample_batchnorm0_gamma is done with shape:  (1024,)\n",
      "layer3_downsample_batchnorm0_beta is done with shape:  (1024,)\n",
      "layer3_downsample_batchnorm0_running_mean is done with shape:  (1024,)\n",
      "layer3_downsample_batchnorm0_running_var is done with shape:  (1024,)\n",
      "layer3_1_conv0_weight is done with shape:  (256, 1024, 1, 1, 1)\n",
      "layer3_1_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_1_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_1_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_1_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_1_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_1_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_1_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_1_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_1_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_1_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_1_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_1_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_1_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_1_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer3_2_conv0_weight is done with shape:  (256, 1024, 3, 1, 1)\n",
      "layer3_2_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_2_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_2_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_2_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_2_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_2_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_2_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_2_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_2_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_2_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_2_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_2_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_2_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_2_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer3_3_conv0_weight is done with shape:  (256, 1024, 1, 1, 1)\n",
      "layer3_3_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_3_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_3_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_3_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_3_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_3_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_3_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_3_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_3_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_3_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_3_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_3_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_3_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_3_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer3_4_conv0_weight is done with shape:  (256, 1024, 3, 1, 1)\n",
      "layer3_4_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_4_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_4_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_4_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_4_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_4_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_4_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_4_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_4_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_4_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_4_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_4_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_4_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_4_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer3_5_conv0_weight is done with shape:  (256, 1024, 1, 1, 1)\n",
      "layer3_5_batchnorm0_gamma is done with shape:  (256,)\n",
      "layer3_5_batchnorm0_beta is done with shape:  (256,)\n",
      "layer3_5_batchnorm0_running_mean is done with shape:  (256,)\n",
      "layer3_5_batchnorm0_running_var is done with shape:  (256,)\n",
      "layer3_5_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
      "layer3_5_batchnorm1_gamma is done with shape:  (256,)\n",
      "layer3_5_batchnorm1_beta is done with shape:  (256,)\n",
      "layer3_5_batchnorm1_running_mean is done with shape:  (256,)\n",
      "layer3_5_batchnorm1_running_var is done with shape:  (256,)\n",
      "layer3_5_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
      "layer3_5_batchnorm2_gamma is done with shape:  (1024,)\n",
      "layer3_5_batchnorm2_beta is done with shape:  (1024,)\n",
      "layer3_5_batchnorm2_running_mean is done with shape:  (1024,)\n",
      "layer3_5_batchnorm2_running_var is done with shape:  (1024,)\n",
      "layer4_0_conv0_weight is done with shape:  (512, 1024, 1, 1, 1)\n",
      "layer4_0_batchnorm0_gamma is done with shape:  (512,)\n",
      "layer4_0_batchnorm0_beta is done with shape:  (512,)\n",
      "layer4_0_batchnorm0_running_mean is done with shape:  (512,)\n",
      "layer4_0_batchnorm0_running_var is done with shape:  (512,)\n",
      "layer4_0_conv1_weight is done with shape:  (512, 512, 1, 3, 3)\n",
      "layer4_0_batchnorm1_gamma is done with shape:  (512,)\n",
      "layer4_0_batchnorm1_beta is done with shape:  (512,)\n",
      "layer4_0_batchnorm1_running_mean is done with shape:  (512,)\n",
      "layer4_0_batchnorm1_running_var is done with shape:  (512,)\n",
      "layer4_0_conv2_weight is done with shape:  (2048, 512, 1, 1, 1)\n",
      "layer4_0_batchnorm2_gamma is done with shape:  (2048,)\n",
      "layer4_0_batchnorm2_beta is done with shape:  (2048,)\n",
      "layer4_0_batchnorm2_running_mean is done with shape:  (2048,)\n",
      "layer4_0_batchnorm2_running_var is done with shape:  (2048,)\n",
      "layer4_downsample_conv0_weight is done with shape:  (2048, 1024, 1, 1, 1)\n",
      "layer4_downsample_batchnorm0_gamma is done with shape:  (2048,)\n",
      "layer4_downsample_batchnorm0_beta is done with shape:  (2048,)\n",
      "layer4_downsample_batchnorm0_running_mean is done with shape:  (2048,)\n",
      "layer4_downsample_batchnorm0_running_var is done with shape:  (2048,)\n",
      "layer4_1_conv0_weight is done with shape:  (512, 2048, 3, 1, 1)\n",
      "layer4_1_batchnorm0_gamma is done with shape:  (512,)\n",
      "layer4_1_batchnorm0_beta is done with shape:  (512,)\n",
      "layer4_1_batchnorm0_running_mean is done with shape:  (512,)\n",
      "layer4_1_batchnorm0_running_var is done with shape:  (512,)\n",
      "layer4_1_conv1_weight is done with shape:  (512, 512, 1, 3, 3)\n",
      "layer4_1_batchnorm1_gamma is done with shape:  (512,)\n",
      "layer4_1_batchnorm1_beta is done with shape:  (512,)\n",
      "layer4_1_batchnorm1_running_mean is done with shape:  (512,)\n",
      "layer4_1_batchnorm1_running_var is done with shape:  (512,)\n",
      "layer4_1_conv2_weight is done with shape:  (2048, 512, 1, 1, 1)\n",
      "layer4_1_batchnorm2_gamma is done with shape:  (2048,)\n",
      "layer4_1_batchnorm2_beta is done with shape:  (2048,)\n",
      "layer4_1_batchnorm2_running_mean is done with shape:  (2048,)\n",
      "layer4_1_batchnorm2_running_var is done with shape:  (2048,)\n",
      "layer4_2_conv0_weight is done with shape:  (512, 2048, 1, 1, 1)\n",
      "layer4_2_batchnorm0_gamma is done with shape:  (512,)\n",
      "layer4_2_batchnorm0_beta is done with shape:  (512,)\n",
      "layer4_2_batchnorm0_running_mean is done with shape:  (512,)\n",
      "layer4_2_batchnorm0_running_var is done with shape:  (512,)\n",
      "layer4_2_conv1_weight is done with shape:  (512, 512, 1, 3, 3)\n",
      "layer4_2_batchnorm1_gamma is done with shape:  (512,)\n",
      "layer4_2_batchnorm1_beta is done with shape:  (512,)\n",
      "layer4_2_batchnorm1_running_mean is done with shape:  (512,)\n",
      "layer4_2_batchnorm1_running_var is done with shape:  (512,)\n",
      "layer4_2_conv2_weight is done with shape:  (2048, 512, 1, 1, 1)\n",
      "layer4_2_batchnorm2_gamma is done with shape:  (2048,)\n",
      "layer4_2_batchnorm2_beta is done with shape:  (2048,)\n",
      "layer4_2_batchnorm2_running_mean is done with shape:  (2048,)\n",
      "layer4_2_batchnorm2_running_var is done with shape:  (2048,)\n",
      "dense0_weight is skipped with shape:  (101, 2048)\n",
      "dense0_bias is skipped with shape:  (101,)\n",
      "I3D_ResNetV1(\n",
      "  (first_stage): HybridSequential(\n",
      "    (0): Conv3D(3 -> 64, kernel_size=(5, 7, 7), stride=(2, 2, 2), padding=(2, 3, 3), bias=False)\n",
      "    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "    (2): Activation(relu)\n",
      "    (3): MaxPool3D(size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCDHW)\n",
      "  )\n",
      "  (pool2): MaxPool3D(size=(2, 1, 1), stride=(2, 1, 1), padding=(0, 0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCDHW)\n",
      "  (res_layers): HybridSequential(\n",
      "    (0): HybridSequential(\n",
      "      (0): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(64 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        )\n",
      "        (conv1): Conv3D(64 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        )\n",
      "        (conv1): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        )\n",
      "        (conv1): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "    )\n",
      "    (1): HybridSequential(\n",
      "      (0): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(256 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        )\n",
      "        (conv1): Conv3D(256 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv3D(256 -> 512, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        )\n",
      "        (conv1): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(512 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        )\n",
      "        (conv1): Conv3D(512 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        )\n",
      "        (conv1): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "    )\n",
      "    (2): HybridSequential(\n",
      "      (0): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(512 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(512 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv3D(512 -> 1024, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=1024)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "    )\n",
      "    (3): HybridSequential(\n",
      "      (0): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        )\n",
      "        (conv1): Conv3D(1024 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        (relu): Activation(relu)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv3D(1024 -> 2048, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=2048)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(2048 -> 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        )\n",
      "        (conv1): Conv3D(2048 -> 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (conv2): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (bottleneck): HybridSequential(\n",
      "          (0): Conv3D(2048 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "          (5): Activation(relu)\n",
      "          (6): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        )\n",
      "        (conv1): Conv3D(2048 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (conv2): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        (relu): Activation(relu)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (st_avg): GlobalAvgPool3D(size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCDHW)\n",
      "  (head): HybridSequential(\n",
      "    (0): Dropout(p = 0.8, axes=())\n",
      "    (1): Dense(2048 -> 101, linear)\n",
      "  )\n",
      "  (fc): Dense(2048 -> 101, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# Get the model i3d_resnet50_v1_ucf101 with 101 output classes, without pre-trained weights\n",
    "net = get_model(name='i3d_resnet50_v1_ucf101', nclass=101)\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation and Data Loader\n",
    "---------------------------------\n",
    "\n",
    "Data augmentation for video is different from image. For example, if you\n",
    "want to randomly crop a video sequence, you need to make sure all the video\n",
    "frames in this sequence undergo the same cropping process. We provide a\n",
    "new set of transformation functions, working with multiple images.\n",
    "Please checkout the `video.py <../../../gluoncv/data/transforms/video.py>`_ for more details.\n",
    "Most video data augmentation strategies used here are introduced in [Wang15]_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # Fix the input video frames size as 256340 and randomly sample the cropping width and height from\n",
    "    # {256,224,192,168}. After that, resize the cropped regions to 224  224.\n",
    "    video.VideoMultiScaleCrop(size=(224, 224), scale_ratios=[1.0, 0.875, 0.75, 0.66]),\n",
    "    # Randomly flip the video frames horizontally\n",
    "    video.VideoRandomHorizontalFlip(),\n",
    "    # Transpose the video frames from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    video.VideoToTensor(),\n",
    "    # Normalize the video frames with mean and standard deviation calculated across all images\n",
    "    video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the transform functions, we can define data loaders for our\n",
    "training datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 9537 training samples.\n"
     ]
    }
   ],
   "source": [
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 5\n",
    "# Number of data loader workers\n",
    "num_workers = 1\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training the model.\n",
    "# ``new_length`` indicates the number of frames we use as input.\n",
    "# ``new_step`` indicates we skip one frame to sample the input data.\n",
    "train_dataset = UCF101(train=True, new_length=32, new_step=2, transform=transform_train)\n",
    "print('Load %d training samples.' % len(train_dataset))\n",
    "train_data = gluon.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                   shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, Loss and Metric\n",
    "--------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [30, 55, 70, np.inf]\n",
    "\n",
    "# Stochastic gradient descent\n",
    "optimizer = 'sgd'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.01, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize our model, we need a loss function.\n",
    "For classification tasks, we usually use softmax cross entropy as the\n",
    "loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we use accuracy as the metric to monitor our training\n",
    "process. Besides, we record metric values, and will print them at the\n",
    "end of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "After all the preparations, we can finally start training!\n",
    "Following is the script.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In order to finish the tutorial quickly, we only train for 3 epochs on a tiny subset of UCF101,\n",
    "  and 100 iterations per epoch. In your experiments, we recommend setting ``epochs=100`` for the full UCF101 dataset.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 0] train=0.108458 loss=4.251682 time: 191.863132\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 1] train=0.105473 loss=4.081599 time: 175.502793\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 2] train=0.155224 loss=3.703812 time: 170.540126\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 3] train=0.163184 loss=3.600180 time: 169.073131\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 4] train=0.177114 loss=3.446559 time: 171.947413\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 5] train=0.229851 loss=3.218885 time: 169.732693\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 6] train=0.264677 loss=2.966086 time: 169.339666\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 7] train=0.270647 loss=3.002487 time: 170.329425\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 8] train=0.305473 loss=2.818503 time: 168.841974\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 9] train=0.331343 loss=2.692057 time: 167.034223\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 10] train=0.332338 loss=2.727958 time: 171.006250\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 11] train=0.327363 loss=2.735223 time: 167.786795\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 12] train=0.371144 loss=2.553587 time: 169.481323\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 13] train=0.336318 loss=2.559117 time: 165.851628\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 14] train=0.404975 loss=2.342173 time: 166.097803\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 15] train=0.360199 loss=2.451228 time: 164.442074\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 16] train=0.392040 loss=2.389224 time: 164.396395\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 17] train=0.406965 loss=2.366261 time: 162.302660\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 18] train=0.432836 loss=2.188890 time: 162.002438\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 19] train=0.459701 loss=2.074069 time: 161.977880\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 20] train=0.449751 loss=2.125693 time: 162.299891\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 21] train=0.478607 loss=1.999329 time: 159.042040\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 22] train=0.468657 loss=2.095321 time: 158.149569\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 23] train=0.455721 loss=2.061842 time: 156.788542\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 24] train=0.479602 loss=2.032371 time: 155.720372\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 25] train=0.481592 loss=1.926760 time: 156.326155\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 26] train=0.499502 loss=1.857612 time: 158.985597\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 27] train=0.519403 loss=1.826217 time: 158.423461\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 28] train=0.559204 loss=1.674466 time: 157.298598\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 29] train=0.558209 loss=1.677078 time: 157.095822\n",
      "New learning rate:  0.001\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 30] train=0.610945 loss=1.421874 time: 156.632598\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 31] train=0.672637 loss=1.196427 time: 155.944129\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 32] train=0.679602 loss=1.121016 time: 156.420385\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 33] train=0.692537 loss=1.107807 time: 157.404650\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 34] train=0.720398 loss=1.023415 time: 156.798751\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 35] train=0.712438 loss=1.033251 time: 157.084083\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 36] train=0.715423 loss=0.987528 time: 157.713653\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 37] train=0.717413 loss=0.941117 time: 160.403106\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 38] train=0.720398 loss=0.954218 time: 158.924316\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 39] train=0.739303 loss=0.887508 time: 160.686789\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 40] train=0.740299 loss=0.864906 time: 160.871786\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 41] train=0.738308 loss=0.849372 time: 159.286993\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 42] train=0.740299 loss=0.820048 time: 161.057569\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 43] train=0.754229 loss=0.884886 time: 161.083432\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 44] train=0.751244 loss=0.822034 time: 160.664295\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 45] train=0.741294 loss=0.875419 time: 162.745705\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 46] train=0.759204 loss=0.824796 time: 161.665911\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 47] train=0.788060 loss=0.709271 time: 164.773325\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 48] train=0.790050 loss=0.693499 time: 163.732987\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 49] train=0.776119 loss=0.732558 time: 164.216734\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 50] train=0.770149 loss=0.693333 time: 168.133010\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 51] train=0.783085 loss=0.713441 time: 167.580544\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 52] train=0.770149 loss=0.751344 time: 167.031145\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 53] train=0.800995 loss=0.683351 time: 168.775580\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 54] train=0.793035 loss=0.688869 time: 172.419033\n",
      "New learning rate:  0.0001\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 55] train=0.775124 loss=0.754615 time: 168.914020\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 56] train=0.801990 loss=0.656452 time: 166.783481\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 57] train=0.805970 loss=0.659548 time: 167.673903\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 58] train=0.800000 loss=0.673255 time: 169.137472\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 59] train=0.807960 loss=0.659551 time: 168.577705\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 60] train=0.805970 loss=0.650316 time: 165.952478\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 61] train=0.783085 loss=0.715877 time: 166.306499\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 62] train=0.807960 loss=0.629108 time: 166.058625\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 63] train=0.820896 loss=0.632218 time: 166.942132\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 64] train=0.778109 loss=0.736178 time: 166.042361\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 65] train=0.787065 loss=0.723106 time: 167.857428\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 66] train=0.805970 loss=0.654776 time: 169.949442\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 67] train=0.797015 loss=0.679274 time: 169.327460\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 68] train=0.786070 loss=0.694235 time: 168.700178\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 69] train=0.822886 loss=0.608413 time: 171.455552\n",
      "New learning rate:  1e-05\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 70] train=0.828856 loss=0.565684 time: 174.200033\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 71] train=0.802985 loss=0.661835 time: 169.975022\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 72] train=0.795025 loss=0.679475 time: 169.414952\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 73] train=0.803980 loss=0.638162 time: 169.299986\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 74] train=0.793035 loss=0.690635 time: 167.077525\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 75] train=0.792040 loss=0.692334 time: 170.151582\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 76] train=0.821891 loss=0.561711 time: 170.720190\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 77] train=0.802985 loss=0.596859 time: 169.963927\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 78] train=0.813930 loss=0.650949 time: 169.408561\n",
      ".........................................................................................................................................................................................................|\n",
      "[Epoch 79] train=0.805970 loss=0.636056 time: 172.368502\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 80\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        new_lr = trainer.learning_rate*lr_decay\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        print(\"New learning rate: \", new_lr)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        print(\".\", end=\"\")\n",
    "        # Extract data and label\n",
    "        data = split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = []\n",
    "            for _, X in enumerate(data):\n",
    "                X = X.reshape((-1,) + X.shape[2:])\n",
    "                pred = net(X)\n",
    "                output.append(pred)\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.mean().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "        if i == 200:\n",
    "            print(\"|\")\n",
    "            break\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([acc])\n",
    "    print('[Epoch %d] train=%f loss=%f time: %f' %\n",
    "        (epoch, acc, train_loss / (i+1), time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "train_history.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. Joao Carreira and Andrew Zisserman. Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. In Computer Vision and Pattern Recognition (CVPR), 2017.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
